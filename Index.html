<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <title>üíôShameem's Personal Site</title>
  </head>
  <body style="background-color: #f4eeed;">
    <!-- <img src="https://scontent.fdel5-1.fna.fbcdn.net/v/t31.0-1/c438.94.1172.1172a/s160x160/286023_10150244118526782_2306082_o.jpg?_nc_cat=103&ccb=2&_nc_sid=dbb9e7&_nc_ohc=MetYQjIE1TkAX97aLNn&_nc_ht=scontent.fdel5-1.fna&tp=28&oh=ec45a3b833651662a83bda6dfd03679a&oe=601DC6AF" alt="Shameem profile picture">
      -->
      <table cellspacing="20">
        <tr>
          <td>
            <img src="Images/MSK_IDBI_circle-cropped.png" alt="Shameem profile picture">
          </td>
          <td>
            <h1><a href="https://in.linkedin.com/in/mohammad-shameem-khan-4b887815">Mohd. Shameem Khan</a></h1>
            <p><em>Specialist Data Engineering</em></p>
            <p>Shameem Khan is Specialist Platform having 16+ years of experience in Azure & Google Cloud, Big Data,
              BI and Data Warehousing, especially in Real Estate, Retails, Banking, Sales and Marketing, Broadcasting Tele media and
              Telecommunications industry. He has excellent experience as an overall solution architect with respect to Azure cloud setup,
              Big Data Solutions, Artificial Intelligence Model creation, Customer Data Platform, Data Migration, Churn Data Management,
              Customer Fraud Detection, Recommendation Engine provisioning with front end application using React Native, Java Scripting,
              and Python.</p>
            <p>Shameem has strong analytical skills, "big picture" thinker, proactive and adaptive in nature.
              He worked across all levels in data technology which includes Full stack frontend and backend, AI, Survey Programming,
              Data Cleansing, ETL, Database Scripting, PL/SQL programming, Data Modelling, BI Reporting, Data Visualizations and
              Data Engineering/Sciences.</p>
            <p>Shameem was in Colorado & Atlanta, US from July-2008 to Dec-2008 and Feb-2018 to Jan-2019 & Calgary, CANADA from
              Jan-2010 to Dec-2013 on onsite assignment.</p>
          </td>
        </tr>
      </table>

    <hr>
    <h3>Work Experience</h3>
    <ul>
      <li>Specialist Data Engineering, Publicissapient, Noida, India from Oct‚Äô 09 ‚Äì Till Date.</li>
      <li>Developer Xavient Software Solutions, Noida, India  from Oct‚Äô 06 ‚Äì Sep‚Äô 09.</li>
      <li>Data Analyst Greenfield Online, Gurgaon, India  from Aug‚Äô 04 ‚Äì Sep‚Äô 06.</li>
      <li>Trainee Software Engineer NISCAIR, New Delhi, India from Jan‚Äô 04 ‚Äì May‚Äô 04.</li>
    </ul>
    <hr>
    <h3>Experience Details</h3>
    <ol>
      <li>Sapient Corporation, Februrary 2019 to September 2020.</li>
      <p><em>Solution Architect</em> ‚Äì Sapient Nex-Gen Platform Setup on Azure Cloud.</p>
      <p>The goal of this project was to create an integrated platform which gives 360 degree vision of employee, client, profit &
         margin, hiring, talent review. Also, it should serve as a ground availability for different recommendation engine using
         AI feature enablement. This solution includes infrastructure creation, tools evaluation, capacity planning, development
          & deployment approach for more than 5+ sources like Smart Recruiter, Time Tracking, Enrichment DB, MR Reporting,
          Herald and Salesforce 360. Solution includes key feature like 360 degree insight on capability available in Sapient,
          Real-time interface to know about open needs and allocation of individual at real-time, AI solution on potential client
          win/loss etc.</p>
      <li>Sapient Corporation, Februrary 2018 to Februrary 2019.</li>
      <p><em>Principle Data Scientist</em>‚Äì UPS CDP.</p>
      <p>The goal of this project is create a Customer Data Platform with BIG Data solution that includes infrastructure creation,
        tools evaluation, capacity planning, development & deployment approach for more than 10+ sources like Datorama, Infolib,
        Doubleclick, Webtrends, AAM and other internal systems like DWH, Infolib, Flex Analytics. CDP Solution includes key
        feature like Customer Data Stitching, Digital Marketting Trends, and Expense & Revenue and Predictive analytics solution.
        </p>

      <li>Sapient Corporation, December 2014 to January 2018.</li>
      <p>Architect cum Data Modeler ‚Äì Cadillac Fairview EDW.</p>
      <p>The goal of this project is create a BIG Data solution and BI Data warehouse on cloud environment from scratch which
        includes infrastructure creation, tools evaluation, capacity planning, development & deployment approach for more than
        50+ sources. We have successfully integrated more than 10 sources and have solution emplaced around customer‚Äôs 360
        degree journey, customer behavior and customer sentiments for different geography. </p>

      <li>Sapient Corporation, March 2017 to September 2017.</li>
      <p>Data Modeler ‚Äì STR Historical Migration to CMRS.</p>
      <p>The goal of this project was to migrate all historical trades data of banking firm from STR (existing application) to
        CMRS (new application) ensuring all the netigrity remains intact. So that if in future if there is any audit then then it
         can be done from new application without going into old applications which is supposed to get decomssioned after
         successful migration.</p>

      <li>Sapient Corporation, October 2009 to November 2014.</li>
      <p>Senior Associate Platform ‚Äì FGL Sport‚Äôs BI-Steady State.</p>
      <p>The goal of the project is to maintain the existing state of Forzani CDW environment. Whenever any production failures
        happen then it needs to be resolved as quickly as possible. In FGL, the SLA is 10 min to respond when ever any issue
        occurs. Also, this project involves new enhancement and its maintenance. The big challenges that we faced around incorrect
        data upload, performance, hardware upgrade issue etc. We use to look into business concern query and get the data analyzed
        based on their needs. If it reflects incorrect data then we need to look for its root cause and get it resolved through
        RFC.</p>

      <li>Xavient Software Solutions, January 2009 to September 2009.</li>
      <p>ETL Developer ‚Äì Dishnetwork‚Äôs Geo Phase IV.</p>
      <p>The goal of the project is to create a single repository for all addresses and include an association of addresses to
        people (subscribers), to demographic information (Equifax), party data and maintain history of key elements. In the
        existing system, the addresses are landed in numerous databases and there is not one ‚Äúsingle source of the truth‚Äù for
        an address.  This results in physical storage issues because of maintaining duplicate address data.  The various address
        information are been managed by different vendors and can‚Äôt be compared in the existing systems which results to loss in
        revenue when any campaign or advertisement is made. Even, there were reports that same bill is been delivered by
        Dishnetworks several times. Also, apart from CSG loads all the processes needs manual intervention which increases the
        possibility of manual monitoring which should be avoided.</p>

      <li>Xavient Software Solutions, November 2008 to December 2008.</li>
      <p>ETL Developer ‚Äì T-Mobile Corporation‚Äôs Flex pay BI Reporting.</p>
      <p>The purpose of this project is to populate the Flex pay related source system data into the Teradata Data warehouse.
        This data is further used by the business users for reporting purposes.</p>

      <li>Xavient Software Solutions, September 2008 to November 2008.</li>
      <p>ETL Developer ‚Äì T-Mobile Corporation‚Äôs TED Data warehouse.</p>
      <p>It was a time based project. The purpose of this project to manage the OLAP database (Teradata) maintained;
        modified to add new requirements, improve the performance of existing processes. Identify the erroneous records and
        missing data.</p>

      <li>Xavient Software Solutions, January 2008 to August 2008.</li>
      <p>ETL Developer ‚Äì EchoStar‚Äôs Metadata Phase -2.</p>
      <p>This project was enhancement of metadata phase 1. In this phase client wants to integrate the metadata of all the DDLs,
        DMLs, Batch scripts, NT Hyperion Report information, Informatica metadata information into metadata repository by
        building a relationship in between each metadata information.</p>

      <li>Xavient Software Solutions, October 2007 to December 2007.</li>
      <p>ETL Developer ‚Äì Echostar‚Äôs Offer Product.</p>
      <p>This project was built for sales and marketing people for identifying the least or most popular packages used by the
        subscribers so that they could change the schemes according to the subscriber needs and could increase the revenue by
        doing necessary changes in the schemes. Identification of offers and promotion of product is been made by building
        relationship with geographical location, population, service provider, and demographics.</p>

      <li>Xavient Software Solutions, August 2007 to October 2007.</li>
      <p>ETL Developer ‚Äì Echostar‚Äôs Teradata Acceller.</p>
      <p>The purpose of this project was to load the data provided by vendor named ACCELER which do marketing for the EchoStar
        product. For each subscription Acceller gets paid so, EchoStar wants to keep track of the all subscription made by them
        to keep track of all the details. To achieve this objective, this system has been emplaced.</p>

      <li>Xavient Software Solutions, October 2006 to July 2007.</li>
      <p>ETL Developer ‚Äì Echostar‚Äôs Metadata Repository Phase1.</p>
      <p>The purpose of the Metadata Repository Phase 1 is to integrate the metadata information of all the existing batch
        scripts with HR based information and RFC/SCR details of old team track and the new team track and made available
        on the EchoStar Portal.</p>

      <li>Greenfield Online (Now Toluna), July 2005 to September 2006.</li>
      <p>Data Analyst ‚Äì $MME Company‚Äôs Sales Analysis System.</p>
      <p>This project is aimed at evaluation of sales and functional performance of the business. This project gives the
        information on customer‚Äôs buying patterns and trend over a period of time. The data mart details, different locations
        and its desirable to produce automatically, periodically set of standard reports & query. The business team analysis for
        further improvement of their business use OLAP reporting. Find out where it has got good strengths to release new
        products without spending much for advertisement. Find out seasonal products with respect to the location and produces
        the product in large volumes.The main achievement of this project is to improve the sales by identifying key factors like
        the total sales revenue in Click stream Data and Point-of-sales (POS) systems, and also to find the number of customers
        reviewed the product but not ordered, how many customers ordered, then cancelled, reasons.</p>

      <li>Greenfield Online (Now Toluna), August 2004 to June 2005.</li>
      <p>Data Analyst ‚Äì San Miguel Corporation‚Äôs Sales Information System.</p>
      <p>San Miguel Corporation (SMC), Philippines is the largest publicly listed food, beverage and packaging company in
        Philippines. SMC's food and agribusiness units make animal feed, coconut oil, dairy and meat products.
        The scope of this Project is to capture the Sales and Marketing processes and data warehouse to maintain
        clients‚Äô history and current data. Data from Operational Source System was extracted, transformed and loaded
        into data warehouse using Informatica.</p>
    </ol>
    <hr>
    <a href="education.html">Education</a>
    <a href="contact.html">Contact</a>
    <a href="skills.html">Skills</a>
    <a href="skillrating.html">Skills Rating</a>
    <br>
  </body>
</html>
